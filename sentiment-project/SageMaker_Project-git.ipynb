{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"conda_python3","language":"python","name":"conda_python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"name":"SageMaker_Project.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"oXFMwXDKScCa"},"source":["# Creating a Sentiment Analysis Web App\n","## Using PyTorch and SageMaker\n","\n","_Deep Learning Nanodegree Program | Deployment_\n","\n","---\n","\n","Construct a complete project from end to end. The goal will be to have a simple web page which a user can use to enter a movie review. The web page will then send the review off to our deployed model which will predict the sentiment of the entered review.\n","\n","\n","## General Outline\n","\n","\n","1. Download or otherwise retrieve the data.\n","2. Process / Prepare the data.\n","3. Upload the processed data to S3.\n","4. Train a chosen model.\n","5. Test the trained model (typically using a batch transform job).\n","6. Deploy the trained model.\n","7. Use the deployed model.\n"]},{"cell_type":"code","metadata":{"id":"KbI19jVqScCr","outputId":"582900f5-d079-48bc-99cd-0b17ee6c172e"},"source":["# Make sure that we use SageMaker 1.x\n","!pip install sagemaker==1.72.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sagemaker==1.72.0\n","  Using cached sagemaker-1.72.0-py2.py3-none-any.whl\n","Collecting smdebug-rulesconfig==0.1.4\n","  Using cached smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n","Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n","Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.17.35)\n","Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.15.2)\n","Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n","Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.9)\n","Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.7.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.4)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n","Requirement already satisfied: botocore<1.21.0,>=1.20.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.20.35)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3>=1.14.12->sagemaker==1.72.0) (1.26.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n","Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n","Installing collected packages: smdebug-rulesconfig, sagemaker\n","  Attempting uninstall: smdebug-rulesconfig\n","    Found existing installation: smdebug-rulesconfig 1.0.1\n","    Uninstalling smdebug-rulesconfig-1.0.1:\n","      Successfully uninstalled smdebug-rulesconfig-1.0.1\n","  Attempting uninstall: sagemaker\n","    Found existing installation: sagemaker 2.31.1\n","    Uninstalling sagemaker-2.31.1:\n","      Successfully uninstalled sagemaker-2.31.1\n","Successfully installed sagemaker-1.72.0 smdebug-rulesconfig-0.1.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SUVpjbZpScCu"},"source":["## Step 1: Downloading the data\n","\n","As in the XGBoost in SageMaker notebook, we will be using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n","\n","> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."]},{"cell_type":"code","metadata":{"id":"zbCthoiyScCv","outputId":"edc2445e-2162-4014-e223-a76263a32009"},"source":["%mkdir ../data\n","!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘../data’: File exists\n","--2021-04-08 09:18:14--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘../data/aclImdb_v1.tar.gz’\n","\n","../data/aclImdb_v1. 100%[===================>]  80.23M  24.0MB/s    in 4.2s    \n","\n","2021-04-08 09:18:18 (18.9 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y8SubX3mScCw"},"source":["## Step 2: Preparing and Processing the data\n","\n","Also, as in the XGBoost notebook, we will be doing some initial data processing."]},{"cell_type":"code","metadata":{"id":"QV9Z9lxvScCx"},"source":["import os\n","import glob\n","\n","def read_imdb_data(data_dir='../data/aclImdb'):\n","    data = {}\n","    labels = {}\n","    \n","    for data_type in ['train', 'test']:\n","        data[data_type] = {}\n","        labels[data_type] = {}\n","        \n","        for sentiment in ['pos', 'neg']:\n","            data[data_type][sentiment] = []\n","            labels[data_type][sentiment] = []\n","            \n","            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n","            files = glob.glob(path)\n","            \n","            for f in files:\n","                with open(f) as review:\n","                    data[data_type][sentiment].append(review.read())\n","                    # Here we represent a positive review by '1' and a negative review by '0'\n","                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n","                    \n","            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n","                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n","                \n","    return data, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DPe24s4DScCy","outputId":"b4b6aad1-cbe5-4d79-c3ed-cc721b4bfa40"},"source":["data, labels = read_imdb_data()\n","print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n","            len(data['train']['pos']), len(data['train']['neg']),\n","            len(data['test']['pos']), len(data['test']['neg'])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XNdnfrpIScCz"},"source":["Now that we've read the raw training and testing data from the downloaded dataset, we will combine the positive and negative reviews and shuffle the resulting records."]},{"cell_type":"code","metadata":{"id":"AHLYwFhkScC0"},"source":["from sklearn.utils import shuffle\n","\n","def prepare_imdb_data(data, labels):\n","    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n","    \n","    #Combine positive and negative reviews and labels\n","    data_train = data['train']['pos'] + data['train']['neg']\n","    data_test = data['test']['pos'] + data['test']['neg']\n","    labels_train = labels['train']['pos'] + labels['train']['neg']\n","    labels_test = labels['test']['pos'] + labels['test']['neg']\n","    \n","    #Shuffle reviews and corresponding labels within training and test sets\n","    data_train, labels_train = shuffle(data_train, labels_train)\n","    data_test, labels_test = shuffle(data_test, labels_test)\n","    \n","    # Return a unified training data, test data, training labels, test labets\n","    return data_train, data_test, labels_train, labels_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p7R3ZRSgScC2","outputId":"e08038ef-c039-4076-d983-74ddafdcfce9"},"source":["train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n","print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["IMDb reviews (combined): train = 25000, test = 25000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u3B4va9fScC3"},"source":["Now that we have our training and testing sets unified and prepared, we should do a quick check and see an example of the data our model will be trained on. "]},{"cell_type":"code","metadata":{"id":"oKqNylo2ScC4","outputId":"43a9bfc0-9c1e-45c2-8ef5-384121a668f6"},"source":["print(train_X[100])\n","#print(train_y[100])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I gotta say, Clive Barker's Undying is by far the best horror game to have ever been made. I've played Resident Evil, Silent Hill and the Evil Dead and Castlevania games but none of them have captured the pure glee with which this game tackles its horrific elements. Barker is good at what he does, which is attach the horror to our world, and it shows as his hand is clearly everywhere in this game. Heck, even his voice is in the game as one of the main characters. Full of lush visuals and enough atmosphere to shake a stick at, Undying is the game to beat in my books as the best horror title. I just wish that this had made it to a console system but alas poor PC sales nipped that one in the bud.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U8oCej-0ScC5"},"source":["The first step in processing the reviews is to make sure that any html tags that appear should be removed."]},{"cell_type":"code","metadata":{"id":"vWr4RNblScC6"},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import *\n","\n","import re\n","from bs4 import BeautifulSoup\n","\n","def review_to_words(review):\n","    nltk.download(\"stopwords\", quiet=True)\n","    stemmer = PorterStemmer()\n","    \n","    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n","    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n","    words = text.split() # Split string into words\n","    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n","    words = [PorterStemmer().stem(w) for w in words] # stem\n","    \n","    return words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P6874rCxScC7"},"source":["The `review_to_words` method defined above uses `BeautifulSoup` to remove any html tags that appear and uses the `nltk` package to tokenize the reviews. As a check to ensure we know how everything is working, try applying `review_to_words` to one of the reviews in the training set."]},{"cell_type":"code","metadata":{"id":"_chzNgf-ScC7","outputId":"b862ab58-0b04-4b3d-cab6-7df2c8cc259e"},"source":["# TODO: Apply review_to_words to a review (train_X[100] or any other review)\n","output = review_to_words(train_X[100])\n","print(output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['gotta', 'say', 'clive', 'barker', 'undi', 'far', 'best', 'horror', 'game', 'ever', 'made', 'play', 'resid', 'evil', 'silent', 'hill', 'evil', 'dead', 'castlevania', 'game', 'none', 'captur', 'pure', 'glee', 'game', 'tackl', 'horrif', 'element', 'barker', 'good', 'attach', 'horror', 'world', 'show', 'hand', 'clearli', 'everywher', 'game', 'heck', 'even', 'voic', 'game', 'one', 'main', 'charact', 'full', 'lush', 'visual', 'enough', 'atmospher', 'shake', 'stick', 'undi', 'game', 'beat', 'book', 'best', 'horror', 'titl', 'wish', 'made', 'consol', 'system', 'ala', 'poor', 'pc', 'sale', 'nip', 'one', 'bud']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oe83FUpPScC9"},"source":["The method below applies the `review_to_words` method to each of the reviews in the training and testing datasets. In addition it caches the results."]},{"cell_type":"code","metadata":{"id":"mKI3RU0HScC-"},"source":["import pickle\n","\n","cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n","os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n","\n","def preprocess_data(data_train, data_test, labels_train, labels_test,\n","                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n","    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n","\n","    # If cache_file is not None, try to read from it first\n","    cache_data = None\n","    if cache_file is not None:\n","        try:\n","            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n","                cache_data = pickle.load(f)\n","            print(\"Read preprocessed data from cache file:\", cache_file)\n","        except:\n","            pass  # unable to read from cache, but that's okay\n","    \n","    # If cache is missing, then do the heavy lifting\n","    if cache_data is None:\n","        # Preprocess training and test data to obtain words for each review\n","        #words_train = list(map(review_to_words, data_train))\n","        #words_test = list(map(review_to_words, data_test))\n","        words_train = [review_to_words(review) for review in data_train]\n","        words_test = [review_to_words(review) for review in data_test]\n","        \n","        # Write to cache file for future runs\n","        if cache_file is not None:\n","            cache_data = dict(words_train=words_train, words_test=words_test,\n","                              labels_train=labels_train, labels_test=labels_test)\n","            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n","                pickle.dump(cache_data, f)\n","            print(\"Wrote preprocessed data to cache file:\", cache_file)\n","    else:\n","        # Unpack data loaded from cache file\n","        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n","                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n","    \n","    return words_train, words_test, labels_train, labels_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLfmaBDdScDA","outputId":"8361a8bf-8238-492a-981a-8064e7e0f062"},"source":["# Preprocess data\n","train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Read preprocessed data from cache file: preprocessed_data.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nLRd3XpiScDB","outputId":"7aaf455a-6baa-4528-d032-56c813dc20ff"},"source":["len(test_X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25000"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"CyJ_mQ9TScDD"},"source":["### Create a word dictionary\n","\n","To begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the 'no word' and 'infrequent' categories) to be `5000` but you may wish to change this to see how it affects the model.\n","\n","> Complete the implementation for the `build_dict()` method below. Note that even though the vocab_size is set to `5000`, we only want to construct a mapping for the most frequently appearing `4998` words. This is because we want to reserve the special labels `0` for 'no word' and `1` for 'infrequent word'."]},{"cell_type":"code","metadata":{"id":"HAveomlbScDD"},"source":["import numpy as np\n","\n","def build_dict(data, vocab_size = 5000):\n","    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n","    \n","    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n","    #       sentence is a list of words.\n","    \n","    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n","    \n","    wordlist = []        # all words in a single list\n","    [wordlist.append(word) for list in train_X for word in list]\n","    uniquewordlist = []  # all unique words in a single list\n","    freqlist = []        # list of frequency of unique words\n","    \n","    for word in wordlist:\n","        if word not in uniquewordlist:\n","            uniquewordlist.append(word)\n","            freqlist.append(1)\n","        else:\n","            freqlist[uniquewordlist.index(word)] = freqlist[uniquewordlist.index(word)] + 1\n","    \n","    word_count[\"word\"] = uniquewordlist\n","    word_count[\"freq\"] = freqlist\n","    \n","    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n","    #       sorted_words[-1] is the least frequently appearing word.\n","    \n","    sorted_words = None\n","    sorted_freqlist, sorted_words = zip(*sorted(zip(freqlist, uniquewordlist), reverse=True))\n","    \n","    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n","    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n","        word_dict[word] = idx + 2                              # 'infrequent' labels\n","        \n","    return word_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnoYwLtkScDE"},"source":["# Custom - Retrieve word_dict from cache_dir if word_dict already built in previous log-in\n","cache_dir = '../cache/sentiment_analysis'  # where to store cache files\n","os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n","\n","with open(os.path.join(cache_dir, 'word_dict.pkl'), \"rb\") as f:\n","    word_dict = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sm3hUC7UScDE"},"source":["word_dict = build_dict(train_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSfIppOhScDF","outputId":"dd78e954-ddac-4649-b5f5-1cca1bf375a8"},"source":["# TODO: Use this space to determine the five most frequently appearing words in the training set.\n","freq_words = list(word_dict.keys())\n","print(freq_words[0:20]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["['movi', 'film', 'one', 'like', 'time', 'good', 'make', 'charact', 'get', 'see', 'watch', 'stori', 'even', 'would', 'realli', 'well', 'scene', 'look', 'show', 'much']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tXu2umiHScDG"},"source":["### Save `word_dict`\n","\n","Later on when we construct an endpoint which processes a submitted review we will need to make use of the `word_dict` which we have created. As such, we will save it to a file now for future use."]},{"cell_type":"code","metadata":{"id":"LM7aSP2UScDH","outputId":"36b5b4bc-9265-4760-c2b5-a17e4769db55"},"source":["# Custom - Store word_dict in cache_dir\n","cache_dir = '../cache/sentiment_analysis'  # where to store cache files\n","os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n","\n","with open(os.path.join(cache_dir, 'word_dict.pkl'), \"wb\") as f:\n","    pickle.dump(word_dict, f)         "],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'dict'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LbAPnVFOScDI"},"source":["data_dir = '../data/pytorch' # The folder we will use for storing data\n","if not os.path.exists(data_dir): # Make sure that the folder exists\n","    os.makedirs(data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMPmO5RYScDI"},"source":["with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n","    pickle.dump(word_dict, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4oHsDcQScDJ"},"source":["### Transform the reviews\n","\n","Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is `500`."]},{"cell_type":"code","metadata":{"id":"8z7W3WFWScDJ"},"source":["def convert_and_pad(word_dict, sentence, pad=500):\n","    NOWORD = 0 # We will use 0 to represent the 'no word' category\n","    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n","    \n","    working_sentence = [NOWORD] * pad\n","    \n","    for word_index, word in enumerate(sentence[:pad]):\n","        if word in word_dict:\n","            working_sentence[word_index] = word_dict[word]\n","        else:\n","            working_sentence[word_index] = INFREQ\n","            \n","    return working_sentence, min(len(sentence), pad)\n","\n","def convert_and_pad_data(word_dict, data, pad=500):\n","    result = []\n","    lengths = []\n","    \n","    for sentence in data:\n","        converted, leng = convert_and_pad(word_dict, sentence, pad)\n","        result.append(converted)\n","        lengths.append(leng)\n","        \n","    return np.array(result), np.array(lengths)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ley5ZZ8yScDK"},"source":["train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n","test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJ_BzV4UScDL"},"source":["As a quick check to make sure that things are working as intended, check to see what one of the reviews in the training set looks like after having been processeed. Does this look reasonable? What is the length of a review in the training set?"]},{"cell_type":"code","metadata":{"id":"p8tMvvh0ScDL","outputId":"5ec307be-a053-48dd-f942-ed46874ecd25"},"source":["# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n","print(train_X[0])\n","print(train_X_len[0])\n","# Seems to have worked alright. The number of non-zero elements - the indices of words from the word_dict list -\n","# is the same as the length of the review, which is intended."],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 304  242   98 3057   32  833  307  263  260   26   40  300    2    1\n","  489    2 2937 1469    6    1    1   57 3537   60    1    1  264   43\n","  143   53    2 1074 1142   93  169    2 2261  144  845  370    1   46\n","  777 1682  795   13  602    1    1   87  484 1102   55  732    1  327\n","   28  277 2501   10  435   18 1946 1101  697    1  602 3015    1    2\n","    1 2501    2  111  279 1946 1011 3206  489    1 1246    2  317 3817\n","    1 3817 1739    1    2   18 1114    2 4451  689  574  382  130   11\n"," 4658    2  123    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0]\n","101\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ossVOEyGScDN"},"source":["## Step 3: Upload the data to S3\n","\n","### Save the processed training dataset locally\n","\n","It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form `label`, `length`, `review[500]` where `review[500]` is a sequence of `500` integers representing the words in the review."]},{"cell_type":"code","metadata":{"id":"zKFDb5DQScDN"},"source":["import pandas as pd\n","    \n","pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n","        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xaFws4yScDO"},"source":["### Uploading the training data\n","\n","\n","Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model."]},{"cell_type":"code","metadata":{"id":"8oSUmv7_ScDO"},"source":["import sagemaker\n","\n","sagemaker_session = sagemaker.Session()\n","\n","bucket = sagemaker_session.default_bucket()\n","prefix = 'sagemaker/sentiment_rnn'\n","\n","role = sagemaker.get_execution_role()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfVB1yQQScDP"},"source":["input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"advTmcMyScDQ"},"source":["## Step 4: Build and Train the PyTorch Model\n","\n","We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we have provided the necessary model object in the `model.py` file, inside of the `train` folder. You can see the provided implementation by running the cell below."]},{"cell_type":"code","metadata":{"id":"SStvKiiJScDQ","outputId":"119b1624-1661-4ce6-b514-6f7f2c51d62d"},"source":["!pygmentize train/model.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n","\r\n","\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n","    \u001b[33m\"\"\"\u001b[39;49;00m\r\n","\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n","\u001b[33m    \"\"\"\u001b[39;49;00m\r\n","\r\n","    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n","        \u001b[33m\"\"\"\u001b[39;49;00m\r\n","\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n","\u001b[33m        \"\"\"\u001b[39;49;00m\r\n","        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n","\r\n","        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n","        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n","        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n","        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n","        \r\n","        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m\r\n","\r\n","    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n","        \u001b[33m\"\"\"\u001b[39;49;00m\r\n","\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n","\u001b[33m        \"\"\"\u001b[39;49;00m\r\n","        x = x.t()\r\n","        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n","        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n","        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n","        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n","        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n","        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n","        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_h_4qdKaScDR"},"source":["import torch\n","import torch.utils.data\n","\n","# Read in only the first 250 rows\n","train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n","\n","# Turn the input pandas dataframe into tensors\n","train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n","train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n","\n","# Build the dataset\n","train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n","# Build the dataloader\n","train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrrBPTMiScDS"},"source":["### Writing the training method\n","\n","Next we need to write the training code itself. This should be very similar to training methods that you have written before to train PyTorch models. We will leave any difficult aspects such as model saving / loading and parameter loading until a little later."]},{"cell_type":"code","metadata":{"id":"qGBKe6vCScDS"},"source":["def train(model, train_loader, epochs, optimizer, loss_fn, device):\n","    for epoch in range(1, epochs + 1):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_loader:         \n","            batch_X, batch_y = batch\n","            \n","            batch_X = batch_X.to(device)\n","            batch_y = batch_y.to(device)\n","            \n","            # TODO: Complete this train method to train the model provided.\n","            output_batch = model(batch_X)  # compute model output\n","            loss = loss_fn(output_batch.squeeze(), batch_y)  # calculate loss\n","            optimizer.zero_grad()                  # clear previous gradients\n","            loss.backward()                        # compute gradients of all variables wrt loss\n","            optimizer.step()                       # perform updates using calculated gradients\n","            \n","            total_loss += loss.data.item()\n","        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkEiXyR2ScDT","outputId":"05d80f6d-20ab-4807-dcd3-e418b27d36b0"},"source":["import torch.optim as optim\n","from train.model import LSTMClassifier\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = LSTMClassifier(32, 100, 5000).to(device)\n","optimizer = optim.Adam(model.parameters())\n","loss_fn = torch.nn.BCELoss()\n","\n","train(model, train_sample_dl, 5, optimizer, loss_fn, device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1, BCELoss: 0.6900425314903259\n","Epoch: 2, BCELoss: 0.6786326766014099\n","Epoch: 3, BCELoss: 0.6677982568740845\n","Epoch: 4, BCELoss: 0.6547678947448731\n","Epoch: 5, BCELoss: 0.6372617363929749\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Ow0JoyYScDU"},"source":["### Training the model\n"]},{"cell_type":"code","metadata":{"id":"SJoPH7EgScDU"},"source":["from sagemaker.pytorch import PyTorch\n","\n","estimator = PyTorch(entry_point=\"train.py\",\n","                    source_dir=\"train\",\n","                    role=role,\n","                    framework_version='0.4.0',\n","                    train_instance_count=1,\n","                    train_instance_type='ml.m4.xlarge', # original ml.p2.xlarge, unauthorized to request AWS\n","                    hyperparameters={\n","                        'epochs': 10,\n","                        'hidden_dim': 200,\n","                    })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKrOZQvtScDU","outputId":"1968e20e-9de1-44e3-8f8c-22057f3a455e"},"source":["estimator.fit({'training': input_data})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n","'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n","'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"],"name":"stderr"},{"output_type":"stream","text":["2021-04-08 10:38:07 Starting - Starting the training job...\n","2021-04-08 10:38:09 Starting - Launching requested ML instances.........\n","2021-04-08 10:39:39 Starting - Preparing the instances for training......\n","2021-04-08 10:41:04 Downloading - Downloading input data...\n","2021-04-08 10:41:24 Training - Downloading the training image.................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n","\u001b[34mbash: no job control in this shell\u001b[0m\n","\u001b[34m2021-04-08 10:44:16,682 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n","\u001b[34m2021-04-08 10:44:16,685 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n","\u001b[34m2021-04-08 10:44:16,699 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n","\u001b[34m2021-04-08 10:44:16,710 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n","\u001b[34m2021-04-08 10:44:17,348 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n","\u001b[34mGenerating setup.py\u001b[0m\n","\u001b[34m2021-04-08 10:44:17,349 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n","\u001b[34m2021-04-08 10:44:17,349 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n","\u001b[34m2021-04-08 10:44:17,349 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n","\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n","\u001b[34mProcessing /opt/ml/code\u001b[0m\n","\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n","  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n","\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\n","  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n","\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n","  Downloading https://files.pythonhosted.org/packages/8c/1c/c0981ef85165eb739c10f2b24d7729cef066b2bc220fbd1dd0d3c67df39a/nltk-3.6.1-py3-none-any.whl (1.5MB)\u001b[0m\n","\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\u001b[0m\n","\u001b[34m  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\u001b[0m\n","\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n","  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n","\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n","  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\u001b[0m\n","\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n","\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\u001b[0m\n","\u001b[34m  Downloading https://files.pythonhosted.org/packages/38/3f/4c42a98c9ad7d08c16e7d23b2194a0e4f3b2914662da8bc88986e4e6de1f/regex-2021.4.4.tar.gz (693kB)\u001b[0m\n","\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n","  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\u001b[0m\n","\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n","  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n","\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n","\u001b[34mCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4->-r requirements.txt (line 4))\n","  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n","\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n","  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n","\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n","\u001b[34mBuilding wheels for collected packages: train, regex\n","  Running setup.py bdist_wheel for train: started\u001b[0m\n","\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-n81xwniu/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n","  Running setup.py bdist_wheel for regex: started\u001b[0m\n","\n","2021-04-08 10:44:14 Training - Training image download completed. Training in progress.\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n","  Stored in directory: /root/.cache/pip/wheels/c9/05/a8/b85fa0bd7850b99f9b4f106972975f2e3c46412e12f9949b58\u001b[0m\n","\u001b[34mSuccessfully built train regex\u001b[0m\n","\u001b[34mInstalling collected packages: pytz, numpy, pandas, regex, tqdm, joblib, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n","  Found existing installation: numpy 1.15.4\n","    Uninstalling numpy-1.15.4:\u001b[0m\n","\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n","\u001b[34mSuccessfully installed beautifulsoup4-4.9.3 html5lib-1.1 joblib-0.14.1 nltk-3.6.1 numpy-1.18.5 pandas-0.24.2 pytz-2021.1 regex-2021.4.4 soupsieve-2.1 tqdm-4.60.0 train-1.0.0 webencodings-0.5.1\u001b[0m\n","\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n","\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","\u001b[34m2021-04-08 10:44:39,700 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n","\u001b[34m2021-04-08 10:44:39,715 sagemaker-containers INFO     Invoking user script\n","\u001b[0m\n","\u001b[34mTraining Env:\n","\u001b[0m\n","\u001b[34m{\n","    \"num_cpus\": 4,\n","    \"log_level\": 20,\n","    \"hosts\": [\n","        \"algo-1\"\n","    ],\n","    \"input_config_dir\": \"/opt/ml/input/config\",\n","    \"output_data_dir\": \"/opt/ml/output/data\",\n","    \"input_dir\": \"/opt/ml/input\",\n","    \"additional_framework_parameters\": {},\n","    \"channel_input_dirs\": {\n","        \"training\": \"/opt/ml/input/data/training\"\n","    },\n","    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n","    \"resource_config\": {\n","        \"hosts\": [\n","            \"algo-1\"\n","        ],\n","        \"network_interface_name\": \"eth0\",\n","        \"current_host\": \"algo-1\"\n","    },\n","    \"module_dir\": \"s3://sagemaker-us-east-1-286722933665/sagemaker-pytorch-2021-04-08-10-38-07-376/source/sourcedir.tar.gz\",\n","    \"network_interface_name\": \"eth0\",\n","    \"hyperparameters\": {\n","        \"hidden_dim\": 200,\n","        \"epochs\": 10\n","    },\n","    \"input_data_config\": {\n","        \"training\": {\n","            \"TrainingInputMode\": \"File\",\n","            \"S3DistributionType\": \"FullyReplicated\",\n","            \"RecordWrapperType\": \"None\"\n","        }\n","    },\n","    \"job_name\": \"sagemaker-pytorch-2021-04-08-10-38-07-376\",\n","    \"model_dir\": \"/opt/ml/model\",\n","    \"output_dir\": \"/opt/ml/output\",\n","    \"num_gpus\": 0,\n","    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n","    \"current_host\": \"algo-1\",\n","    \"user_entry_point\": \"train.py\",\n","    \"module_name\": \"train\"\u001b[0m\n","\u001b[34m}\n","\u001b[0m\n","\u001b[34mEnvironment variables:\n","\u001b[0m\n","\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n","\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n","\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n","\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n","\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n","\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n","\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n","\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-286722933665/sagemaker-pytorch-2021-04-08-10-38-07-376/source/sourcedir.tar.gz\u001b[0m\n","\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n","\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n","\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n","\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n","\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n","\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n","\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n","\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n","\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n","\u001b[34mSM_NUM_GPUS=0\u001b[0m\n","\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n","\u001b[34mSM_NUM_CPUS=4\u001b[0m\n","\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-04-08-10-38-07-376\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-286722933665/sagemaker-pytorch-2021-04-08-10-38-07-376/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n","\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n","\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n","\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n","\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n","\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n","\u001b[34mSM_MODULE_NAME=train\n","\u001b[0m\n","\u001b[34mInvoking script with the following command:\n","\u001b[0m\n","\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n","\n","\u001b[0m\n","\u001b[34mUsing device cpu.\u001b[0m\n","\u001b[34mGet train data loader.\u001b[0m\n","\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34mEpoch: 1, BCELoss: 0.6694670854782572\u001b[0m\n","\u001b[34mEpoch: 2, BCELoss: 0.6274341515132359\u001b[0m\n","\u001b[34mEpoch: 3, BCELoss: 0.5674002048920612\u001b[0m\n","\u001b[34mEpoch: 4, BCELoss: 0.49718912462798914\u001b[0m\n","\u001b[34mEpoch: 5, BCELoss: 0.43789916500753284\u001b[0m\n","\u001b[34mEpoch: 6, BCELoss: 0.39180668641109856\u001b[0m\n","\u001b[34mEpoch: 7, BCELoss: 0.3598738257982293\u001b[0m\n","\u001b[34mEpoch: 8, BCELoss: 0.32745734769470836\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"em2LklRdScDV"},"source":["**Custom Note**: As seen above the training appears to have stopped at Epoch 8. Whereas, it has been completed as confirmed on Cloudwatch training logs. The notebook session had timed out which is why the display update had stopped working. It took 2 hours for the training to complete. The reason being - training instance 'ml.m4.xlarge' used instead of 'ml.p2.xlarge'. For unkown reasons I am unauthorized to create a support request on AWS support to request an instance limit increase. I had raised 2 support tickets, but the issue still hasn't been resolved. Hence, I had to improvise and move ahead with the instance available at hand."]},{"cell_type":"markdown","metadata":{"id":"y01uWkVnScDW"},"source":["## Step 5: Testing the model\n","\n","As mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly.\n","\n","## Step 6: Deploy the model for testing\n","\n"," Deploy the trained model."]},{"cell_type":"code","metadata":{"id":"CYvUwTr9ScDX","outputId":"40ead060-cbb5-41b3-b3e6-7d862fac7a6c"},"source":["# TODO: Deploy the trained model\n","predictor = estimator.deploy(instance_type='ml.m4.xlarge',\n","                                     initial_instance_count=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n","'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"],"name":"stderr"},{"output_type":"stream","text":["-----------------!"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DdW6wXI7ScDY"},"source":["## Step 7 - Use the model for testing\n","\n","Once deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is."]},{"cell_type":"code","metadata":{"id":"RMerwwYkScDY"},"source":["test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCc7EcZhScDZ"},"source":["# We split the data into chunks and send each chunk seperately, accumulating the results.\n","\n","def predict(data, rows=512):\n","    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n","    predictions = np.array([])\n","    for array in split_array:\n","        predictions = np.append(predictions, predictor.predict(array))\n","    \n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oin_MsHKScDZ"},"source":["predictions = predict(test_X.values)\n","predictions = [round(num) for num in predictions]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SlUBtNZoScDa","outputId":"0662e25b-9136-4207-a484-99f02d9492b0"},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(test_y, predictions)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.84132"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"hVOJjk7EScDc"},"source":["###  More testing\n","\n","We now have a trained model which has been deployed and which we can send processed reviews to and which returns the predicted sentiment. However, ultimately we would like to be able to send our model an unprocessed review. That is, we would like to send the review itself as a string. For example, suppose we wish to send the following review to our model."]},{"cell_type":"code","metadata":{"id":"HHTeISVUScDc"},"source":["test_review = 'The simplest pleasures in life are the awesome, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6B358R5EScDd"},"source":["The question we now need to answer is, how do we send this review to our model?\n","\n","Recall in the first section of this notebook we did a bunch of data processing to the IMDb dataset. In particular, we did two specific things to the provided reviews.\n"," - Removed any html tags and stemmed the input\n"," - Encoded the review as a sequence of integers using `word_dict`\n"," \n","In order process the review we will need to repeat these two steps.\n","\n","Using the `review_to_words` and `convert_and_pad` methods from section one, convert `test_review` into a numpy array `test_data` suitable to send to our model. Remember that our model expects input of the form `review_length, review[500]`."]},{"cell_type":"code","metadata":{"id":"J0hlDlbDScDd"},"source":["# TODO: Convert test_review into a form usable by the model and save the results in test_data\n","test_data = None\n","\n","test_words = review_to_words(test_review)\n","test_X_single, test_X_single_len = convert_and_pad(word_dict, test_words)\n","test_data = np.array([np.array([test_X_single_len] + test_X_single)]) # needs to be arrayed\n","#test_data = pd.concat([pd.DataFrame(test_X_single_len), pd.DataFrame(test_X_single)], axis=1) #won't work"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMt5v_nwScDe"},"source":["Now that we have processed the review, we can send the resulting array to our model to predict the sentiment of the review."]},{"cell_type":"code","metadata":{"id":"SpiZ4T3pScDe","outputId":"f07a6103-09f5-4cfa-94ab-cd729003dcbb"},"source":["predictor.predict(test_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(0.87232673, dtype=float32)"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"Fhejw6raScDf"},"source":["Since the return value of our model is close to `1`, we can be certain that the review we submitted is positive."]},{"cell_type":"markdown","metadata":{"id":"CmreKMn2ScDf"},"source":["### Delete the endpoint\n","\n","Of course, just like in the XGBoost notebook, once we've deployed an endpoint it continues to run until we tell it to shut down. Since we are done using our endpoint for now, we can delete it."]},{"cell_type":"code","metadata":{"id":"BpDKk5mSScDg","outputId":"5c5c957b-e396-4d5d-951d-f3e582ba8f9d"},"source":["estimator.delete_endpoint()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"C6YFOzUSScDg","outputId":"3c9881fb-e2f7-43ff-d452-1d379e32e6f9"},"source":["#Store the model information so you can call \n","print(estimator.model_data)\n","print(role)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["s3://sagemaker-us-east-1-286722933665/sagemaker-pytorch-2021-04-08-10-38-07-376/output/model.tar.gz\n","arn:aws:iam::286722933665:role/service-role/AmazonSageMaker-ExecutionRole-20210324T230648\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GflJ2A3CScDh","outputId":"be8f0622-14e5-4a9a-accc-82a56c248d31"},"source":["#print(estimator.model_data)\n","print(role)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["arn:aws:iam::286722933665:role/service-role/AmazonSageMaker-ExecutionRole-20210324T230648\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9ENIrGEIScDi"},"source":["## Step 6 (again) - Deploy the model for the web app\n","\n","### (TODO) Writing inference code\n","\n","Before writing our custom inference code, we will begin by taking a look at the code which has been provided."]},{"cell_type":"code","metadata":{"id":"TOhsRjklScDj","outputId":"cc3d4d2e-01f3-4643-d202-e5cdc24a6cfa"},"source":["!pygmentize serve/predict.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n","\r\n","\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\r\n","\r\n","\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\r\n","\r\n","\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n","    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n","    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n","\r\n","    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n","    model_info = {}\r\n","    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n","        model_info = torch.load(f)\r\n","\r\n","    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n","\r\n","    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n","    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n","    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n","\r\n","    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\r\n","    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n","        model.load_state_dict(torch.load(f))\r\n","\r\n","    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n","    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n","        model.word_dict = pickle.load(f)\r\n","\r\n","    model.to(device).eval()\r\n","\r\n","    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n","    \u001b[34mreturn\u001b[39;49;00m model\r\n","\r\n","\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n","    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n","        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","        \u001b[34mreturn\u001b[39;49;00m data\r\n","    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n","\r\n","\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n","    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n","\r\n","\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n","    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","\r\n","    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n","    \r\n","    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n","        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n","    \r\n","    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\r\n","    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\r\n","    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\r\n","    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\r\n","\r\n","    test_words = review_to_words(input_data)\r\n","    data_X, data_len = convert_and_pad(model.word_dict, test_words)\r\n","\r\n","    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\r\n","    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\r\n","    data_pack = np.hstack((data_len, data_X))\r\n","    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\r\n","    \r\n","    data = torch.from_numpy(data_pack)\r\n","    data = data.to(device)\r\n","\r\n","    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\r\n","    model.eval()\r\n","\r\n","    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\r\n","    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\r\n","\r\n","    result = \u001b[34mNone\u001b[39;49;00m\r\n","    output = model(data)\r\n","    result = np.round(output.numpy())\r\n","\r\n","    \u001b[34mreturn\u001b[39;49;00m result\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jXdICz-pScDk"},"source":["### Deploying the model\n","\n","Now that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container.\n","\n","**NOTE**: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a `numpy` array. In our case we want to send a string so we need to construct a simple wrapper around the `RealTimePredictor` class to accomodate simple strings. In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data."]},{"cell_type":"code","metadata":{"id":"supPNkT2ScDk"},"source":["# Copy model data from previously created model artifact\n","my_model_data = 's3://sagemaker-us-east-1-286722933665/sagemaker-pytorch-2021-04-08-10-38-07-376/output/model.tar.gz'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSDIinfGScDl","outputId":"2f402ce5-c0dc-4593-a06e-3db580d848ef"},"source":["from sagemaker.predictor import RealTimePredictor\n","from sagemaker.pytorch import PyTorchModel\n","\n","class StringPredictor(RealTimePredictor):\n","    def __init__(self, endpoint_name, sagemaker_session):\n","        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n","\n","model = PyTorchModel(model_data=my_model_data,\n","                     role = role,\n","                     framework_version='0.4.0',\n","                     entry_point='predict.py',\n","                     source_dir='serve',\n","                     predictor_cls=StringPredictor)\n","predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n","'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"],"name":"stderr"},{"output_type":"stream","text":["---------------!"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XYCVmfRTScDm"},"source":["### Testing the model\n","\n","Now that we have deployed our model with the custom inference code, we should test to see if everything is working. Here we test our model by loading the first `250` positive and negative reviews and send them to the endpoint, then collect the results. The reason for only sending some of the data is that the amount of time it takes for our model to process the input and then perform inference is quite long and so testing the entire data set would be prohibitive."]},{"cell_type":"code","metadata":{"id":"TJg-vt8EScDm","outputId":"54cc984f-60b3-4daf-d032-8063e034c679"},"source":["print(model.model_data)\n","print(model.role)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["s3://sagemaker-us-east-1-286722933665/sagemaker-pytorch-2021-04-08-10-38-07-376/output/model.tar.gz\n","arn:aws:iam::286722933665:role/service-role/AmazonSageMaker-ExecutionRole-20210324T230648\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SJk7kpxNScDn"},"source":["import glob\n","\n","def test_reviews(data_dir='../data/aclImdb', stop=250):\n","    \n","    results = []\n","    ground = []\n","    \n","    # We make sure to test both positive and negative reviews    \n","    for sentiment in ['pos', 'neg']:\n","        \n","        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n","        files = glob.glob(path)\n","        \n","        files_read = 0\n","        \n","        print('Starting ', sentiment, ' files')\n","        \n","        # Iterate through the files and send them to the predictor\n","        for f in files:\n","            with open(f) as review:\n","                # First, we store the ground truth (was the review positive or negative)\n","                if sentiment == 'pos':\n","                    ground.append(1)\n","                else:\n","                    ground.append(0)\n","                # Read in the review and convert to 'utf-8' for transmission via HTTP\n","                review_input = review.read().encode('utf-8')\n","                # Send the review to the predictor and store the results\n","                results.append(float(predictor.predict(review_input)))\n","                \n","            # Sending reviews to our endpoint one at a time takes a while so we\n","            # only send a small number of reviews\n","            files_read += 1\n","            if files_read == stop:\n","                break\n","            \n","    return ground, results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSf2gYr2ScDo","outputId":"913b4147-b0c9-4087-8916-cf7b4c38fc20"},"source":["ground, results = test_reviews()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting  pos  files\n","Starting  neg  files\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CZ_jGiFaScDo","outputId":"9a45163b-ca69-460d-af9c-08e1ea1b7fa0"},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(ground, results)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.83"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"XbHq0yRmScDp"},"source":["As an additional test, we can try sending the `test_review` that we looked at earlier."]},{"cell_type":"code","metadata":{"id":"ZmNLlXOrScDp","outputId":"696915b3-dd53-47b7-8876-2f3633491219"},"source":["predictor.predict(test_review)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["b'1.0'"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"Py6Sz1B5ScDq"},"source":["Now that we know our endpoint is working as expected, we can set up the web page that will interact with it. If you don't have time to finish the project now, make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back."]},{"cell_type":"markdown","metadata":{"id":"qAMSHLVeScDr"},"source":["## Step 7 (again): Use the model for the web app\n","\n","\n","#### Part A: Create an IAM Role for the Lambda function\n","\n","#### Part B: Create a Lambda function\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"cjGUEdgVScDs","outputId":"8eeb0be0-600e-4241-b4c9-32844405047b"},"source":["predictor.endpoint"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'sagemaker-pytorch-2021-04-09-12-10-35-384'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"iAQicCmkScDs"},"source":["Once you have added the endpoint name to the Lambda function, click on **Save**. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function.\n","\n","### Setting up API Gateway\n"]},{"cell_type":"markdown","metadata":{"id":"mV6dyIa_ScDt"},"source":["## Step 4: Deploying our web app\n"]},{"cell_type":"markdown","metadata":{"id":"9PSkPaxiScDt"},"source":["Now that your web app is working, trying playing around with it and see how well it works."]},{"cell_type":"markdown","metadata":{"id":"M1HiBhD9ScDu"},"source":["### Delete the endpoint\n","\n","Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill."]},{"cell_type":"code","metadata":{"id":"Xhact01MScDu"},"source":["predictor.delete_endpoint()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ek9Ja43GScDv"},"source":[""],"execution_count":null,"outputs":[]}]}